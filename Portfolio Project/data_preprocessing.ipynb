{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSC 510 Portfolio Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Filtering Initial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/karlestes/Documents/Grad School/CSC 510 - Foundations of Artificial Intelligence/Portfolio Project\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import imageio as iio\n",
    "import cv2 as cv\n",
    "import json\n",
    "import math\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "brfss_data_filepath = os.path.join(os.getcwd(), \"Data/LLCP2020.XPT\")\n",
    "brfss_filtered_filepath = os.path.join(os.getcwd(), \"Data/brfss_filtered_data.csv\")\n",
    "\n",
    "oasis_csv_filepath = os.path.join(os.getcwd(),\"Data/oasis_cross-sectional.csv\")\n",
    "oasis_csv_filtered_filepath = os.path.join(os.getcwd(),\"Data/oasis_filtered.csv\")\n",
    "\n",
    "def round_dec(num, places = 2):\n",
    "    \"\"\"Rounds the provided number to a specific number of decimal places (defaulted to 2)\"\"\"\n",
    "\n",
    "    multiplier = 10 ** places\n",
    "\n",
    "    return math.trunc(num * multiplier) / multiplier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BRFSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN TO GET CLEAN BRFSS DATA\n",
    "# Read in the dataset from the file\n",
    "# may take a while\n",
    "print(\"Reading SAS Data\")\n",
    "df=pd.read_sas(brfss_data_filepath)\n",
    "print(\"SAS Data Loaded\")\n",
    "\n",
    "print(\"Removing Unecessary Columns Data\")\n",
    "# Filter only relavant data for analysis\n",
    "df_filtered = df[['_STATE','DISPCODE','SEXVAR','GENHLTH','PHYSHLTH','MENTHLTH','POORHLTH','EXERANY2','SLEPTIM1','CVDINFR4','CVDCRHD4',\n",
    "                  'CVDSTRK3','ASTHMA3','CHCSCNCR','CHCOCNCR','CHCCOPD2','HAVARTH4','ADDEPEV3','CHCKDNY2','DIABETE4','PREGNANT',\n",
    "                  'DEAF','BLIND','DECIDE','DIFFWALK','DIFFDRES','DIFFALON','SMOKE100','USENOW3',\n",
    "                  'FALL12MN','HIVRISK5','TOLDHEPC','HAVEHEPB','CIMEMLOS','CDASSIST','CDSOCIAL',\n",
    "                  'ACEDEPRS','ACEHURT1','ACETOUCH','ACETTHEM','ACEHVSEX','_RACE','_AGE_G','_BMI5CAT','_RFDRHV7']]\n",
    "print(\"Slices of Data Gathered\")\n",
    "print(\"data shape: \",df_filtered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temp copy of original filtered data\n",
    "filtered_copy = df_filtered.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load temp copy\n",
    "df_filtered = filtered_copy.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Filtering\n",
    "dropna_categories = ['SEXVAR','_AGE_G','DEAF','BLIND','USENOW3','CIMEMLOS','_RACE','_BMI5CAT']\n",
    "dontknow_categories = [('PHYSHLTH', 77),('MENTHLTH',77),('POORHLTH',77),('EXERANY2',7),('CVDINFR4',7),('CVDCRHD4',7),('CVDSTRK3',7),('ASTHMA3',7),('CHCSCNCR',7),\n",
    "                       ('CHCOCNCR',7),('CHCCOPD2',7),('HAVARTH4',7),('ADDEPEV3',7),('CHCKDNY2',7),('DIABETE4',7),('DECIDE',7),('DIFFWALK',7),('DIFFDRES',7),\n",
    "                       ('DIFFALON',7),('SMOKE100',7),('HIVRISK5',7),('TOLDHEPC',7),('HAVEHEPB',7),('ACEDEPRS',7),('ACEHURT1',7),('ACETOUCH',7),('ACETTHEM',7),\n",
    "                       ('ACEHVSEX',7),('_RFDRHV7',9),('SLEPTIM1',77),('FALL12MN',77),('GENHLTH',7)]\n",
    "categorical_categories = ['PHYSHLTH','MENTHLTH','POORHLTH','SLEPTIM1','FALL12MN']\n",
    "\n",
    "non_cd_categories = ['_STATE','DISPCODE','SEXVAR','GENHLTH','PHYSHLTH','MENTHLTH','POORHLTH','EXERANY2','SLEPTIM1','CVDINFR4','CVDCRHD4',\n",
    "                  'CVDSTRK3','ASTHMA3','CHCSCNCR','CHCOCNCR','CHCCOPD2','HAVARTH4','ADDEPEV3','CHCKDNY2','DIABETE4','PREGNANT',\n",
    "                  'DEAF','BLIND','DIFFWALK','DIFFDRES','DIFFALON','SMOKE100','USENOW3',\n",
    "                  'FALL12MN','HIVRISK5','TOLDHEPC','HAVEHEPB',\n",
    "                  'ACEDEPRS','ACEHURT1','ACETOUCH','ACETTHEM','ACEHVSEX','_RACE','_AGE_G','_BMI5CAT','_RFDRHV7']\n",
    "\n",
    "# Remove rows missing necessary info\n",
    "print(\"Initial data shape: \", df_filtered.shape)\n",
    "print(\"Removing rows missing necessary datapoints\")\n",
    "df_filtered = df_filtered[df_filtered.DISPCODE == 1100] # Ensure completed interviews only\n",
    "df_filtered = df_filtered.dropna(axis='rows',subset=dropna_categories)\n",
    "print(\"Data shape after removing responses with necessary missing data: \", df_filtered.shape)\n",
    "\n",
    "# Replace blank data with Don't Know/Not Sure\n",
    "print(\"Replacing missing data points with corresponding label for don't know/not sure\")\n",
    "\n",
    "for column,value in dontknow_categories:\n",
    "    df_filtered[column] = df_filtered[column].fillna(value)\n",
    "\n",
    "# Convert categorical data\n",
    "for column in categorical_categories:\n",
    "    masks = []\n",
    "    if column in ['PHYSHLTH','MENTHLTH','POORHLTH']:\n",
    "        masks.append((df_filtered[column] >= 1) & (df_filtered[column] < 7))\n",
    "        masks.append((df_filtered[column] >= 7) & (df_filtered[column] < 14))\n",
    "        masks.append((df_filtered[column] >= 14) & (df_filtered[column] <= 30))\n",
    "    elif column == 'SLEPTIM1':\n",
    "        masks.append((df_filtered.SLEPTIM1 >= 1) & (df_filtered.SLEPTIM1 < 6))\n",
    "        masks.append((df_filtered.SLEPTIM1 >= 6) & (df_filtered.SLEPTIM1 < 12))\n",
    "        masks.append((df_filtered.SLEPTIM1 >= 12) & (df_filtered.SLEPTIM1 < 18))\n",
    "        masks.append((df_filtered.SLEPTIM1 >= 18) & (df_filtered.SLEPTIM1 <= 24))\n",
    "    elif column == 'FALL12MN':\n",
    "        masks.append((df_filtered.FALL12MN >= 1) & (df_filtered.FALL12MN < 10))\n",
    "        masks.append((df_filtered.FALL12MN >= 10) & (df_filtered.FALL12MN < 20))\n",
    "        masks.append((df_filtered.FALL12MN >= 20) & (df_filtered.FALL12MN < 30))\n",
    "        masks.append((df_filtered.FALL12MN >= 30) & (df_filtered.FALL12MN < 40))\n",
    "        masks.append((df_filtered.FALL12MN >= 40) & (df_filtered.FALL12MN < 50))\n",
    "        masks.append((df_filtered.FALL12MN >= 50) & (df_filtered.FALL12MN < 60))\n",
    "        masks.append((df_filtered.FALL12MN >= 60) & (df_filtered.FALL12MN < 70))\n",
    "        masks.append((df_filtered.FALL12MN >= 70) & (df_filtered.FALL12MN <= 76))\n",
    "    \n",
    "    for i,mask in enumerate(masks):\n",
    "        df_filtered.loc[mask, column] = (i+1)\n",
    "\n",
    "# Handling Pregnant Variable\n",
    "df_filtered.loc[df.SEXVAR == 1, 'PREGNANT'] = 2 # All 'M' listed as not pregnant\n",
    "df_filtered.loc[((df._AGE_G >= 4) & (df.PREGNANT != 1)), 'PREGNANT'] = 2 # List anyone 49 and older as not pregnant\n",
    "df_filtered['PREGNANT'] = df_filtered['PREGNANT'].fillna(7) # Fill remaining missing values with don't know/not sure\n",
    "\n",
    "df_filtered = df_filtered.dropna(subset=non_cd_categories)\n",
    "\n",
    "print(\"Data shape after removing responses with any remaining missing data: \", df_filtered.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save filtered data to csv\n",
    "print(\"Saving Filtered Data\")\n",
    "df_filtered.to_csv(brfss_filtered_filepath)\n",
    "print(\"Filtered Data Written to CSV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading filtered data from csv\n",
      "Filtered data loaded\n"
     ]
    }
   ],
   "source": [
    "# RUN TO LOAD FILTERED DATA\n",
    "print(\"Loading filtered data from csv\")\n",
    "df_filtered = pd.read_csv(brfss_filtered_filepath)\n",
    "print(\"Filtered data loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OASIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Original Oasis CSV and get clean filtered data\n",
    "\n",
    "print(\"Loading original CSV\")\n",
    "oasis_df = pd.read_csv(oasis_csv_filepath)\n",
    "print(\"Oasis CSV Shape: \", oasis_df.shape)\n",
    "print(\"Filtering non-CDR labeled subjects\")\n",
    "oasis_df_filtered = oasis_df.dropna(subset=['CDR'])\n",
    "print(\"Oasis CSV Filtered Shape: \", oasis_df_filtered.shape)\n",
    "# Save filtered data\n",
    "\n",
    "print(\"Saving filtered Oasis CSV\")\n",
    "oasis_df_filtered.to_csv(oasis_csv_filtered_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Filtered Oasis CSV\n",
    "oasis_df_filtered = pd.read_csv(oasis_csv_filtered_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move all images to subfolders\n",
    "\n",
    "print(\"Copying all MRI images of CDR patients to corresponding planar subfolders\")\n",
    "\n",
    "coronal_path = os.path.join(os.getcwd(), \"Data/MRI/coronal/\")\n",
    "transverse_path = os.path.join(os.getcwd(), \"Data/MRI/transverse/\")\n",
    "sagittal_path = os.path.join(os.getcwd(), \"Data/MRI/sagittal\")\n",
    "\n",
    "n3_substr = \"_mpr_n3_anon_111_t88_gfc_\"\n",
    "n4_substr = \"_mpr_n4_anon_111_t88_gfc_\"\n",
    "n5_substr = \"_mpr_n5_anon_111_t88_gfc_\"\n",
    "n6_substr = \"_mpr_n6_anon_111_t88_gfc_\"\n",
    "\n",
    "for disc_num in range(1,12): #Look through all twelve disc folders\n",
    "    disc_path = os.path.join(os.getcwd(), \"Data/MRI/disc\" + str(disc_num))\n",
    "    for folder in os.listdir(disc_path): # Check all subject folder names\n",
    "        if folder in oasis_df_filtered['ID'].values: # Check if the folder is in the filtered subject list\n",
    "            subject_data_path = os.path.join(disc_path, folder, \"PROCESSED/MPRAGE/T88_111\")\n",
    "            # Move Coronal\n",
    "            try:\n",
    "               shutil.copy(src=(subject_data_path + \"/\" + folder + n3_substr + \"cor_110.gif\"), dst=(coronal_path))\n",
    "            except:\n",
    "                try:\n",
    "                    shutil.copy(src=(subject_data_path + \"/\" + folder + n4_substr + \"cor_110.gif\"), dst=(coronal_path))\n",
    "                except:\n",
    "                    try:\n",
    "                        shutil.copy(src=(subject_data_path + \"/\" + folder + n5_substr + \"cor_110.gif\"), dst=(coronal_path))\n",
    "                    except:\n",
    "                        try:\n",
    "                            shutil.copy(src=(subject_data_path + \"/\" + folder + n6_substr + \"cor_110.gif\"), dst=(coronal_path))\n",
    "                        except:\n",
    "                            print(f\"ERROR: Could not find coronal image for subject {folder}\")\n",
    "            # Move Transverse\n",
    "            try:\n",
    "               shutil.copy(src=(subject_data_path + \"/\" + folder + n3_substr + \"tra_90.gif\"), dst=(transverse_path))\n",
    "            except:\n",
    "                try:\n",
    "                    shutil.copy(src=(subject_data_path + \"/\" + folder + n4_substr + \"tra_90.gif\"), dst=(transverse_path))\n",
    "                except:\n",
    "                    try:\n",
    "                        shutil.copy(src=(subject_data_path + \"/\" + folder + n5_substr + \"tra_90.gif\"), dst=(transverse_path))\n",
    "                    except:\n",
    "                        try:\n",
    "                            shutil.copy(src=(subject_data_path + \"/\" + folder + n6_substr + \"tra_90.gif\"), dst=(transverse_path))\n",
    "                        except:\n",
    "                            print(f\"ERROR: Could not find transverse image for subject {folder}\")\n",
    "\n",
    "            # Move Sagittal\n",
    "            try:\n",
    "               shutil.copy(src=(subject_data_path + \"/\" + folder + n3_substr + \"sag_95.gif\"), dst=(sagittal_path))\n",
    "            except:\n",
    "                try:\n",
    "                    shutil.copy(src=(subject_data_path + \"/\" + folder + n4_substr + \"sag_95.gif\"), dst=(sagittal_path))\n",
    "                except:\n",
    "                    try:\n",
    "                        shutil.copy(src=(subject_data_path + \"/\" + folder + n5_substr + \"sag_95.gif\"), dst=(sagittal_path))\n",
    "                    except:\n",
    "                        try:\n",
    "                            shutil.copy(src=(subject_data_path + \"/\" + folder + n6_substr + \"sag_95.gif\"), dst=(sagittal_path))\n",
    "                        except:\n",
    "                            print(f\"ERROR: Could not find coronal image for subject {folder}\")\n",
    "\n",
    "print(\"Images copied to planar subfolders\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Error trying to read image .DS_Store\n",
      "Value Error trying to read image .DS_Store\n",
      "Value Error trying to read image .DS_Store\n",
      "Coronal image sizes:  {(176, 176)}\n",
      "Transverse image sizes:  {(208, 176)}\n",
      "Sagittal image sizes:  {(176, 208)}\n"
     ]
    }
   ],
   "source": [
    "# List all image sizes in each folder\n",
    "coronal_path = os.path.join(os.getcwd(), \"Data/MRI/coronal/\")\n",
    "transverse_path = os.path.join(os.getcwd(), \"Data/MRI/transverse/\")\n",
    "sagittal_path = os.path.join(os.getcwd(), \"Data/MRI/sagittal\")\n",
    "\n",
    "paths = [coronal_path,transverse_path,sagittal_path]\n",
    "sizes = []\n",
    "\n",
    "for path in paths:\n",
    "    s = set()\n",
    "    for image in os.listdir(path):\n",
    "        try: \n",
    "            im = iio.get_reader(os.path.join(path,image))\n",
    "            for frame in im:\n",
    "                s.add(frame.shape)\n",
    "        except ValueError:\n",
    "            print(f\"Value Error trying to read image {image}\")\n",
    "\n",
    "    sizes.append(s)\n",
    "\n",
    "print(\"Coronal image sizes: \", sizes[0])\n",
    "print(\"Transverse image sizes: \", sizes[1])\n",
    "print(\"Sagittal image sizes: \", sizes[2])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize all coronal images to 176 by 176 and convert all images from .gif to jpg\n",
    "# Removes the .gif files\n",
    "coronal_path = os.path.join(os.getcwd(), \"Data/MRI/coronal/\")\n",
    "transverse_path = os.path.join(os.getcwd(), \"Data/MRI/transverse/\")\n",
    "sagittal_path = os.path.join(os.getcwd(), \"Data/MRI/sagittal\")\n",
    "\n",
    "paths = [coronal_path, transverse_path, sagittal_path]\n",
    "\n",
    "for path in paths:\n",
    "    for image in os.listdir(path):\n",
    "        new_format = image[:-3]\n",
    "        new_format = new_format + \"jpg\"\n",
    "        try:\n",
    "            im = iio.get_reader(os.path.join(path,image))\n",
    "            for frame in im:\n",
    "\n",
    "                if path == coronal_path:\n",
    "                    resized = cv.resize(frame, (176,176), interpolation=cv.INTER_AREA)\n",
    "                    cv.imwrite(os.path.join(path,new_format), resized)\n",
    "                else:\n",
    "                    cv.imwrite(os.path.join(path,new_format),frame)\n",
    "\n",
    "                os.remove(os.path.join(path,image))\n",
    "        except ValueError:\n",
    "            print(f\"Value Error trying to read image {image}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BRFSS Precomputations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading filtered data from csv\n",
      "Filtered data loaded\n"
     ]
    }
   ],
   "source": [
    "# RUN TO LOAD FILTERED DATA\n",
    "print(\"Loading filtered data from csv\")\n",
    "brfss_filtered = pd.read_csv(brfss_filtered_filepath)\n",
    "print(\"Filtered data loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reported CD Count: 1246\n",
      "Posisble CD Count: 847\n",
      "No CD Count: 59286\n"
     ]
    }
   ],
   "source": [
    "# Data Column for CD Decline\n",
    "brfss_filtered['CD'] = np.where(((brfss_filtered.DECIDE == 1) & ((brfss_filtered.CDASSIST == 1) | (brfss_filtered.CDASSIST == 2) | (brfss_filtered.CDASSIST == 3) | \n",
    "                               (brfss_filtered.CDASSIST == 4)) & ((brfss_filtered.CDSOCIAL == 1) | (brfss_filtered.CDSOCIAL == 2) | (brfss_filtered.CDSOCIAL == 3) | \n",
    "                               (brfss_filtered.CDSOCIAL == 4)) & (brfss_filtered.CIMEMLOS == 1)), True, False)\n",
    "\n",
    "brfss_filtered.loc[brfss_filtered.CD == True, 'CD'] = 1\n",
    "brfss_filtered.loc[brfss_filtered.CD == False, 'CD'] = 0\n",
    "brfss_filtered.loc[((brfss_filtered.DECIDE == 7) | (brfss_filtered.DECIDE == 9)), 'CD'] = 2\n",
    "brfss_filtered.loc[((brfss_filtered.CIMEMLOS == 7) | (brfss_filtered.CIMEMLOS == 9)), 'CD'] = 2\n",
    "brfss_filtered.loc[((brfss_filtered.CDASSIST == 7) | (brfss_filtered.CDASSIST == 9)), 'CD'] = 2\n",
    "brfss_filtered.loc[((brfss_filtered.CDSOCIAL == 7) | (brfss_filtered.CDSOCIAL == 9)), 'CD'] = 2\n",
    "\n",
    "yes_count = np.count_nonzero(brfss_filtered['CD'].values == 1)\n",
    "maybe_count = np.count_nonzero(brfss_filtered['CD'].values == 2)\n",
    "no_count = np.count_nonzero(brfss_filtered['CD'].values == 0)\n",
    "\n",
    "print(f\"Reported CD Count: {yes_count}\\nPosisble CD Count: {maybe_count}\\nNo CD Count: {no_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['_STATE', 'DISPCODE', 'SEXVAR', 'GENHLTH', 'PHYSHLTH', 'MENTHLTH', 'POORHLTH', 'EXERANY2', 'SLEPTIM1', 'CVDINFR4', 'CVDCRHD4', 'CVDSTRK3', 'ASTHMA3', 'CHCSCNCR', 'CHCOCNCR', 'CHCCOPD2', 'HAVARTH4', 'ADDEPEV3', 'CHCKDNY2', 'DIABETE4', 'PREGNANT', 'DEAF', 'BLIND', 'DECIDE', 'DIFFWALK', 'DIFFDRES', 'DIFFALON', 'SMOKE100', 'USENOW3', 'FALL12MN', 'HADHYST2', 'HIVRISK5', 'TOLDHEPC', 'HAVEHEPB', 'CIMEMLOS', 'CDASSIST', 'CDSOCIAL', 'ACEDEPRS', 'ACEHURT1', 'ACETOUCH', 'ACETTHEM', 'ACEHVSEX', '_RACE', '_AGE_G', '_BMI5CAT', '_RFDRHV7'])\n"
     ]
    }
   ],
   "source": [
    "conversions = open(os.path.join(os.getcwd(), \"Data/conversions.json\"))\n",
    "conversions = json.load(conversions)\n",
    "\n",
    "print(conversions.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Beginning likelihood table for _STATE\n",
      "Applying Laplace\n",
      "Could not proccess key: ?\n",
      "Calling pd.crosstab()\n",
      "Adding probability rows and columns\n",
      "Writing table _STATE to csv\n",
      "Table saved\n",
      "\n",
      "Beginning likelihood table for DISPCODE\n",
      "Applying Laplace\n",
      "Calling pd.crosstab()\n",
      "Adding probability rows and columns\n",
      "Writing table DISPCODE to csv\n",
      "Table saved\n",
      "\n",
      "Beginning likelihood table for SEXVAR\n",
      "Applying Laplace\n",
      "Could not proccess key: ?\n",
      "Calling pd.crosstab()\n",
      "Adding probability rows and columns\n",
      "Writing table SEXVAR to csv\n",
      "Table saved\n",
      "\n",
      "Beginning likelihood table for GENHLTH\n",
      "Applying Laplace\n",
      "Could not proccess key: ?\n",
      "Calling pd.crosstab()\n",
      "Adding probability rows and columns\n",
      "Writing table GENHLTH to csv\n",
      "Table saved\n",
      "\n",
      "Beginning likelihood table for PHYSHLTH\n",
      "Applying Laplace\n",
      "Could not proccess key: ?\n",
      "Calling pd.crosstab()\n",
      "Adding probability rows and columns\n",
      "Writing table PHYSHLTH to csv\n",
      "Table saved\n",
      "\n",
      "Beginning likelihood table for MENTHLTH\n",
      "Applying Laplace\n",
      "Could not proccess key: ?\n",
      "Calling pd.crosstab()\n",
      "Adding probability rows and columns\n",
      "Writing table MENTHLTH to csv\n",
      "Table saved\n",
      "\n",
      "Beginning likelihood table for POORHLTH\n",
      "Applying Laplace\n",
      "Could not proccess key: ?\n",
      "Calling pd.crosstab()\n",
      "Adding probability rows and columns\n",
      "Writing table POORHLTH to csv\n",
      "Table saved\n",
      "\n",
      "Beginning likelihood table for EXERANY2\n",
      "Applying Laplace\n",
      "Could not proccess key: ?\n",
      "Calling pd.crosstab()\n",
      "Adding probability rows and columns\n",
      "Writing table EXERANY2 to csv\n",
      "Table saved\n",
      "\n",
      "Beginning likelihood table for SLEPTIM1\n",
      "Applying Laplace\n",
      "Could not proccess key: ?\n",
      "Calling pd.crosstab()\n",
      "Adding probability rows and columns\n",
      "Writing table SLEPTIM1 to csv\n",
      "Table saved\n",
      "\n",
      "Beginning likelihood table for CVDINFR4\n",
      "Applying Laplace\n",
      "Could not proccess key: ?\n",
      "Calling pd.crosstab()\n",
      "Adding probability rows and columns\n",
      "Writing table CVDINFR4 to csv\n",
      "Table saved\n",
      "\n",
      "Beginning likelihood table for CVDCRHD4\n",
      "Applying Laplace\n",
      "Could not proccess key: ?\n",
      "Calling pd.crosstab()\n",
      "Adding probability rows and columns\n",
      "Writing table CVDCRHD4 to csv\n",
      "Table saved\n",
      "\n",
      "Beginning likelihood table for CVDSTRK3\n",
      "Applying Laplace\n",
      "Could not proccess key: ?\n",
      "Calling pd.crosstab()\n",
      "Adding probability rows and columns\n",
      "Writing table CVDSTRK3 to csv\n",
      "Table saved\n",
      "\n",
      "Beginning likelihood table for ASTHMA3\n",
      "Applying Laplace\n",
      "Could not proccess key: ?\n",
      "Calling pd.crosstab()\n",
      "Adding probability rows and columns\n",
      "Writing table ASTHMA3 to csv\n",
      "Table saved\n",
      "\n",
      "Beginning likelihood table for CHCSCNCR\n",
      "Applying Laplace\n",
      "Could not proccess key: ?\n",
      "Calling pd.crosstab()\n",
      "Adding probability rows and columns\n",
      "Writing table CHCSCNCR to csv\n",
      "Table saved\n",
      "\n",
      "Beginning likelihood table for CHCOCNCR\n",
      "Applying Laplace\n",
      "Could not proccess key: ?\n",
      "Calling pd.crosstab()\n",
      "Adding probability rows and columns\n",
      "Writing table CHCOCNCR to csv\n",
      "Table saved\n",
      "\n",
      "Beginning likelihood table for CHCCOPD2\n",
      "Applying Laplace\n",
      "Could not proccess key: ?\n",
      "Calling pd.crosstab()\n",
      "Adding probability rows and columns\n",
      "Writing table CHCCOPD2 to csv\n",
      "Table saved\n",
      "\n",
      "Beginning likelihood table for HAVARTH4\n",
      "Applying Laplace\n",
      "Could not proccess key: ?\n",
      "Calling pd.crosstab()\n",
      "Adding probability rows and columns\n",
      "Writing table HAVARTH4 to csv\n",
      "Table saved\n",
      "\n",
      "Beginning likelihood table for ADDEPEV3\n",
      "Applying Laplace\n",
      "Could not proccess key: ?\n",
      "Calling pd.crosstab()\n",
      "Adding probability rows and columns\n",
      "Writing table ADDEPEV3 to csv\n",
      "Table saved\n",
      "\n",
      "Beginning likelihood table for CHCKDNY2\n",
      "Applying Laplace\n",
      "Could not proccess key: ?\n",
      "Calling pd.crosstab()\n",
      "Adding probability rows and columns\n",
      "Writing table CHCKDNY2 to csv\n",
      "Table saved\n",
      "\n",
      "Beginning likelihood table for DIABETE4\n",
      "Applying Laplace\n",
      "Could not proccess key: ?\n",
      "Calling pd.crosstab()\n",
      "Adding probability rows and columns\n",
      "Writing table DIABETE4 to csv\n",
      "Table saved\n",
      "\n",
      "Beginning likelihood table for PREGNANT\n",
      "Applying Laplace\n",
      "Could not proccess key: ?\n",
      "Calling pd.crosstab()\n",
      "Adding probability rows and columns\n",
      "Writing table PREGNANT to csv\n",
      "Table saved\n",
      "\n",
      "Beginning likelihood table for DEAF\n",
      "Applying Laplace\n",
      "Could not proccess key: ?\n",
      "Calling pd.crosstab()\n",
      "Adding probability rows and columns\n",
      "Writing table DEAF to csv\n",
      "Table saved\n",
      "\n",
      "Beginning likelihood table for BLIND\n",
      "Applying Laplace\n",
      "Could not proccess key: ?\n",
      "Calling pd.crosstab()\n",
      "Adding probability rows and columns\n",
      "Writing table BLIND to csv\n",
      "Table saved\n",
      "\n",
      "Beginning likelihood table for DIFFWALK\n",
      "Applying Laplace\n",
      "Could not proccess key: ?\n",
      "Calling pd.crosstab()\n",
      "Adding probability rows and columns\n",
      "Writing table DIFFWALK to csv\n",
      "Table saved\n",
      "\n",
      "Beginning likelihood table for DIFFDRES\n",
      "Applying Laplace\n",
      "Could not proccess key: ?\n",
      "Calling pd.crosstab()\n",
      "Adding probability rows and columns\n",
      "Writing table DIFFDRES to csv\n",
      "Table saved\n",
      "\n",
      "Beginning likelihood table for DIFFALON\n",
      "Applying Laplace\n",
      "Could not proccess key: ?\n",
      "Calling pd.crosstab()\n",
      "Adding probability rows and columns\n",
      "Writing table DIFFALON to csv\n",
      "Table saved\n",
      "\n",
      "Beginning likelihood table for SMOKE100\n",
      "Applying Laplace\n",
      "Could not proccess key: ?\n",
      "Calling pd.crosstab()\n",
      "Adding probability rows and columns\n",
      "Writing table SMOKE100 to csv\n",
      "Table saved\n",
      "\n",
      "Beginning likelihood table for USENOW3\n",
      "Applying Laplace\n",
      "Could not proccess key: ?\n",
      "Calling pd.crosstab()\n",
      "Adding probability rows and columns\n",
      "Writing table USENOW3 to csv\n",
      "Table saved\n",
      "\n",
      "Beginning likelihood table for FALL12MN\n",
      "Applying Laplace\n",
      "Could not proccess key: ?\n",
      "Calling pd.crosstab()\n",
      "Adding probability rows and columns\n",
      "Writing table FALL12MN to csv\n",
      "Table saved\n",
      "\n",
      "Beginning likelihood table for HIVRISK5\n",
      "Applying Laplace\n",
      "Could not proccess key: ?\n",
      "Calling pd.crosstab()\n",
      "Adding probability rows and columns\n",
      "Writing table HIVRISK5 to csv\n",
      "Table saved\n",
      "\n",
      "Beginning likelihood table for TOLDHEPC\n",
      "Applying Laplace\n",
      "Could not proccess key: ?\n",
      "Calling pd.crosstab()\n",
      "Adding probability rows and columns\n",
      "Writing table TOLDHEPC to csv\n",
      "Table saved\n",
      "\n",
      "Beginning likelihood table for HAVEHEPB\n",
      "Applying Laplace\n",
      "Could not proccess key: ?\n",
      "Calling pd.crosstab()\n",
      "Adding probability rows and columns\n",
      "Writing table HAVEHEPB to csv\n",
      "Table saved\n",
      "\n",
      "Beginning likelihood table for ACEDEPRS\n",
      "Applying Laplace\n",
      "Could not proccess key: ?\n",
      "Calling pd.crosstab()\n",
      "Adding probability rows and columns\n",
      "Writing table ACEDEPRS to csv\n",
      "Table saved\n",
      "\n",
      "Beginning likelihood table for ACEHURT1\n",
      "Applying Laplace\n",
      "Could not proccess key: ?\n",
      "Calling pd.crosstab()\n",
      "Adding probability rows and columns\n",
      "Writing table ACEHURT1 to csv\n",
      "Table saved\n",
      "\n",
      "Beginning likelihood table for ACETOUCH\n",
      "Applying Laplace\n",
      "Could not proccess key: ?\n",
      "Calling pd.crosstab()\n",
      "Adding probability rows and columns\n",
      "Writing table ACETOUCH to csv\n",
      "Table saved\n",
      "\n",
      "Beginning likelihood table for ACETTHEM\n",
      "Applying Laplace\n",
      "Could not proccess key: ?\n",
      "Calling pd.crosstab()\n",
      "Adding probability rows and columns\n",
      "Writing table ACETTHEM to csv\n",
      "Table saved\n",
      "\n",
      "Beginning likelihood table for ACEHVSEX\n",
      "Applying Laplace\n",
      "Could not proccess key: ?\n",
      "Calling pd.crosstab()\n",
      "Adding probability rows and columns\n",
      "Writing table ACEHVSEX to csv\n",
      "Table saved\n",
      "\n",
      "Beginning likelihood table for _RACE\n",
      "Applying Laplace\n",
      "Could not proccess key: ?\n",
      "Calling pd.crosstab()\n",
      "Adding probability rows and columns\n",
      "Writing table _RACE to csv\n",
      "Table saved\n",
      "\n",
      "Beginning likelihood table for _AGE_G\n",
      "Applying Laplace\n",
      "Could not proccess key: ?\n",
      "Calling pd.crosstab()\n",
      "Adding probability rows and columns\n",
      "Writing table _AGE_G to csv\n",
      "Table saved\n",
      "\n",
      "Beginning likelihood table for _BMI5CAT\n",
      "Applying Laplace\n",
      "Could not proccess key: ?\n",
      "Calling pd.crosstab()\n",
      "Adding probability rows and columns\n",
      "Writing table _BMI5CAT to csv\n",
      "Table saved\n",
      "\n",
      "Beginning likelihood table for _RFDRHV7\n",
      "Applying Laplace\n",
      "Could not proccess key: ?\n",
      "Calling pd.crosstab()\n",
      "Adding probability rows and columns\n",
      "Writing table _RFDRHV7 to csv\n",
      "Table saved\n"
     ]
    }
   ],
   "source": [
    "# Precompute Probability Tables\n",
    "non_cd_categories = ['_STATE','DISPCODE','SEXVAR','GENHLTH','PHYSHLTH','MENTHLTH','POORHLTH','EXERANY2','SLEPTIM1','CVDINFR4','CVDCRHD4',\n",
    "                  'CVDSTRK3','ASTHMA3','CHCSCNCR','CHCOCNCR','CHCCOPD2','HAVARTH4','ADDEPEV3','CHCKDNY2','DIABETE4','PREGNANT',\n",
    "                  'DEAF','BLIND','DIFFWALK','DIFFDRES','DIFFALON','SMOKE100','USENOW3',\n",
    "                  'FALL12MN','HIVRISK5','TOLDHEPC','HAVEHEPB',\n",
    "                  'ACEDEPRS','ACEHURT1','ACETOUCH','ACETTHEM','ACEHVSEX','_RACE','_AGE_G','_BMI5CAT','_RFDRHV7']\n",
    "\n",
    "for column in non_cd_categories:\n",
    "    laplace = False\n",
    "    temp_df = brfss_filtered[[column, 'CD']].copy()\n",
    "\n",
    "    print(f\"\\nBeginning likelihood table for {column}\")\n",
    "    # Ensure all keys are added, and apply Laplace if not\n",
    "    # for key in conversions[column].keys():\n",
    "    #     try:\n",
    "    #         n = int(key)\n",
    "\n",
    "    #         if not (n in temp_df[column].values):\n",
    "    #             laplace = True\n",
    "    #             break\n",
    "    #     except:\n",
    "    #         print(f\"Could not convert {key} to integer\")\n",
    "\n",
    "    # Apply laplace as necessary\n",
    "    print(\"Applying Laplace\")\n",
    "    for key in conversions[column].keys():\n",
    "        try:\n",
    "            n = float(key)\n",
    "            if n in temp_df[column].values:\n",
    "                for i in range(3):\n",
    "                    dictionary = {column: n, 'CD': i}\n",
    "                    temp_df = temp_df.append(dictionary, ignore_index=True)\n",
    "        except:\n",
    "            print(f\"Could not proccess key: {key}\")\n",
    "\n",
    "    print(\"Calling pd.crosstab()\")\n",
    "    table = pd.crosstab(temp_df[column], temp_df['CD'], margins=True, margins_name='Total')\n",
    "\n",
    "    # Add row for P(hypothesis)\n",
    "    print(\"Adding probability rows and columns\")\n",
    "    table.loc['P_H'] = [round_dec((table.at['Total',0])/(table.at['Total','Total']),6),\n",
    "                            round_dec((table.at['Total',1])/(table.at['Total','Total']),6),\n",
    "                            round_dec((table.at['Total',2])/(table.at['Total','Total']),6), '']\n",
    "\n",
    "    # Add posterior probabilities\n",
    "    p_d = []\n",
    "    p_d_zero = []\n",
    "    p_d_one = []\n",
    "    p_d_two = []\n",
    "    for index, row in table.iterrows():\n",
    "        try: \n",
    "            p_d.append(round_dec((row['Total'] / table.at['Total','Total']),6))\n",
    "        except:\n",
    "            p_d.append('')\n",
    "\n",
    "        try: \n",
    "            p_d_zero.append(round_dec((row[0] / table.at['Total',0]),6))\n",
    "        except:\n",
    "            p_d.append('')\n",
    "\n",
    "        try: \n",
    "            p_d_one.append(round_dec((row[1] / table.at['Total',1]),6))\n",
    "        except:\n",
    "            p_d.append('')\n",
    "\n",
    "        try: \n",
    "            p_d_two.append(round_dec((row[2] / table.at['Total',2]),6))\n",
    "        except:\n",
    "            p_d.append('')\n",
    "\n",
    "    table['P_D'] = p_d\n",
    "    table['P_D_ZERO'] = p_d_zero\n",
    "    table['P_D_ONE'] = p_d_one\n",
    "    table['P_D_TWO'] = p_d_two\n",
    "\n",
    "    print(f\"Writing table {column} to csv\")\n",
    "    table.to_csv(os.path.join(os.getcwd(), \"Data/tables\", (column + \".csv\")))\n",
    "    print(\"Table saved\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "393fcab780e87c738780ceeb980b543ebdfd57cc9b80e4ffd28fcf595b13429f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('CSUG': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
